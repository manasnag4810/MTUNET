# -*- coding: utf-8 -*-
"""blocks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17HKxjvGX2YpeFOH7YVIHhGWZq-0nMYDq
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class ConvBNReLU(nn.Module):
    def __init__(self, c_in, c_out, k=3, s=1, p=1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(c_in, c_out, k, s, p, bias=False),
            nn.BatchNorm2d(c_out),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        return self.net(x)

class SelfAttention2D(nn.Module):
    def __init__(self, dim, heads=4):
        super().__init__()
        self.heads = heads
        self.scale = (dim // heads) ** -0.5
        self.qkv = nn.Conv2d(dim, dim*3, 1, bias=False)
        self.proj = nn.Conv2d(dim, dim, 1, bias=False)

    def forward(self, x):
        b,c,h,w = x.shape
        qkv = self.qkv(x)
        q,k,v = qkv.chunk(3, dim=1)
        q = rearrange(q, "b (h d) h1 w1 -> b h d (h1 w1)", h=self.heads)
        k = rearrange(k, "b (h d) h1 w1 -> b h d (h1 w1)", h=self.heads)
        v = rearrange(v, "b (h d) h1 w1 -> b h d (h1 w1)", h=self.heads)
        attn = torch.softmax(torch.einsum("bhdi,bhdj->bhij", q*self.scale, k), dim=-1)
        out = torch.einsum("bhij,bhdj->bhdi", attn, v)
        out = rearrange(out, "b h d n -> b (h d) n", h=self.heads)
        out = out.view(b, c, h*w)
        out = out.view(b, c, h, w)
        return self.proj(out)

class FiLM(nn.Module):
    """ Feature wise linear modulation with class logits """
    def __init__(self, num_classes, feat_dim):
        super().__init__()
        self.gamma = nn.Linear(num_classes, feat_dim)
        self.beta  = nn.Linear(num_classes, feat_dim)

    def forward(self, feat, class_logits):
        # class logits to probabilities
        p = torch.softmax(class_logits, dim=1)
        g = self.gamma(p).unsqueeze(-1).unsqueeze(-1)
        b = self.beta(p).unsqueeze(-1).unsqueeze(-1)
        return feat * (1 + g) + b

class UpBlock(nn.Module):
    def __init__(self, c_in, c_skip, c_out, use_attn=True):
        super().__init__()
        self.up = nn.ConvTranspose2d(c_in, c_out, 2, 2)
        self.conv1 = ConvBNReLU(c_out + c_skip, c_out)
        self.conv2 = ConvBNReLU(c_out, c_out)
        self.attn = SelfAttention2D(c_out) if use_attn else nn.Identity()

    def forward(self, x, skip):
        x = self.up(x)
        if x.shape[-1] != skip.shape[-1] or x.shape[-2] != skip.shape[-2]:
            x = F.interpolate(x, size=skip.shape[-2:], mode="bilinear", align_corners=False)
        x = torch.cat([x, skip], dim=1)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.attn(x)
        return x