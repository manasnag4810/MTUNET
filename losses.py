# -*- coding: utf-8 -*-
"""losses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17HKxjvGX2YpeFOH7YVIHhGWZq-0nMYDq
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class DiceLoss(nn.Module):
    def __init__(self, eps=1e-6):
        super().__init__()
        self.eps = eps
    def forward(self, logits, targets):
        probs = torch.sigmoid(logits)
        num = 2 * (probs * targets).sum(dim=(2,3)) + self.eps
        den = (probs + targets).sum(dim=(2,3)) + self.eps
        dice = 1 - (num / den)
        return dice.mean()

class FocalLoss(nn.Module):
    def __init__(self, gamma=2, alpha=0.25, reduction="mean"):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
    def forward(self, logits, targets):
        # binary focal on sigmoid
        p = torch.sigmoid(logits)
        ce = F.binary_cross_entropy_with_logits(logits, targets, reduction="none")
        p_t = p*targets + (1-p)*(1-targets)
        alpha_t = self.alpha*targets + (1-self.alpha)*(1-targets)
        loss = alpha_t * (1-p_t).pow(self.gamma) * ce
        if self.reduction == "mean":
            return loss.mean()
        return loss.sum()

class UncertaintyWeightedMTL(nn.Module):
    """
    L = 1/(2*s_c^2) * L_cls + 1/(2*s_s^2) * L_seg + log(s_c*s_s)
    s_c and s_s are learned
    """
    def __init__(self, init_log_vars=(0.0, 0.0)):
        super().__init__()
        self.log_var_cls = nn.Parameter(torch.tensor(init_log_vars[0]))
        self.log_var_seg = nn.Parameter(torch.tensor(init_log_vars[1]))
    def forward(self, loss_cls, loss_seg):
        w_cls = torch.exp(-self.log_var_cls)
        w_seg = torch.exp(-self.log_var_seg)
        loss = 0.5*(w_cls*loss_cls + w_seg*loss_seg) + 0.5*(self.log_var_cls + self.log_var_seg)
        return loss